Fireworks AI - Fastest Inference for Generative AI Fireworks RFT now available! Fine-tune open models that outperform frontier models. Try today Log In Get Started Build. Tune. Scale. Open-source AI models at blazing speed, optimized for your use case, scaled globally with the Fireworks Inference Cloud Get started Talk to our team Fireworks AI Cloud What Can You Build on Fireworks From experimentation to production, Fireworks provides the platform to build your Generative AI capabilities - optimized and at scale Code Assistance IDE copilots, code generation, debugging agents Learn more Conversational AI Customer support bots, internal helpdesk assistants, multilingual chat Learn more Agentic Systems Multi-step reasoning, planning, and execution pipelines Learn more Search Enterprise assistants, summarization, semantic search, personalized recommendations Learn more Multimedia Text, vision, and speech in real-time workflows Learn more Enterprise RAG Secure, scalable retrieval for knowledge bases and documents Learn more Model library Run the latest open models with a single line of code Fireworks gives you instant access to the most popular OSS models — optimized for cost, speed, and quality on the fastest AI cloud View all models Chronos Hermes 13B v2 $0.2/M Tokens • 4096 Context LLM Gemma 3 27B Instruct $0.9/M Tokens • 131072 Context LLM Qwen3 Coder 480B A35B Instruct $0.45/M Input • $1.8/M Output • 262144 Context LLM GLM-4.6 $0.55/M Input • $2.19/M Output • 202752 Context LLM OpenAI gpt-oss-20b $0.07/M Input • $0.3/M Output • 131072 Context LLM FLUX.1 Kontext Pro $0.04/Image Image Kimi K2 Instruct $0.6/M Input • $2.5/M Output • 131072 Context LLM Segmind Stable Diffusion 1B (SSD-1B) $0.00013/Step Image Whisper V3 Large $0/M Tokens Audio Nous Hermes Llama2 13B $0.2/M Tokens • 4096 Context LLM GLM-4.5 $0.55/M Input • $2.19/M Output • 131072 Context LLM Deepseek R1 05/28 $1.35/M Input • $5.4/M Output • 163840 Context LLM Llama 3 8B Instruct (HF version) $0.2/M Tokens • 8192 Context LLM Chronos Hermes 13B v2 $0.2/M Tokens • 4096 Context LLM Gemma 3 27B Instruct $0.9/M Tokens • 131072 Context LLM Qwen3 Coder 480B A35B Instruct $0.45/M Input • $1.8/M Output • 262144 Context LLM GLM-4.6 $0.55/M Input • $2.19/M Output • 202752 Context LLM Building with Fireworks Run and tune your models on our highly scalable, optimized virtual cloud infrastructure Globally distributed virtual cloud infrastructure running on the latest hardware Enterprise-grade security and reliability across mission-critical workloads Fast inference engine delivering industry-leading throughput and latency. Optimized deployments across quality, speed, and cost Model Lifecycle Management Complete AI model lifecycle management Run the fastest inference, tune with ease, and scale globally, all without managing infrastructure Build Go from idea to output in seconds—with just a prompt. Run the latest open models on Fireworks serverless, with no GPU setup or cold starts. Move to production with on-demand GPUs that auto-scale as you grow Learn more Tune Fine-tune to meet your use case without the complexity. Get the highest-quality results from any open model using advanced tuning techniques like reinforcement learning, quantization-aware tuning, and adaptive speculation Learn more Scale Scale production workloads seamlessly, anywhere, without managing infrastructure. Fireworks automatically provisions AI infrastructure across any deployment type, so you can focus on building Learn more Why Fireworks Startup velocity. Enterprise-grade stability. From AI Natives to Enterprises, Fireworks powers everything from rapid prototyping to mission-critical workloads AI Natives Day 0 support for latest models Highest quality and performance, lowest cost Complete set of developer features no matter where you are on the journey Learn More Enterprise SOC2, HIPAA, and GDPR compliant Bring your own cloud or run on ours Zero data retention and complete data sovereignty Learn More Customer Love What our customers are saying "Fireworks has been a fantastic partner in building AI dev tools at Sourcegraph. Their fast, reliable model inference lets us focus on fine-tuning, AI-powered code search, and deep code context, making Cody the best AI coding assistant. They are responsive and ship at an amazing pace." Beyang Liu | CTO at Sourcegraph "By partnering with Fireworks to fine-tune models, we reduced latency from about 2 seconds to 350 milliseconds, significantly improving performance and enabling us to launch AI features at scale. That improvement is a game changer for delivering reliable, enterprise-scale AI" Sarah Sachs | AI Lead at Notion “Fireworks has been an amazing partner getting our Fast Apply and Copilot++ models running performantly. They exceeded other competitors we reviewed on performance. After testing their quantized model quality for our use cases, we have found minimal degradation. Fireworks helps implement task specific speed ups and new architectures, allowing us to achieve bleeding edge performance!” Sualeh Asif | CPO at Cursor "We've had a really great experience working with Fireworks to host open source models, including SDXL, Llama, and Mistral. After migrating one of our models, we noticed a 3x speedup in response time, which made our app feel much more responsive and boosted our engagement metrics." Spencer Chan | Product Lead at Quora "Fireworks has been a fantastic partner in building AI dev tools at Sourcegraph. Their fast, reliable model inference lets us focus on fine-tuning, AI-powered code search, and deep code context, making Cody the best AI coding assistant. They are responsive and ship at an amazing pace." Beyang Liu | CTO at Sourcegraph "By partnering with Fireworks to fine-tune models, we reduced latency from about 2 seconds to 350 milliseconds, significantly improving performance and enabling us to launch AI features at scale. That improvement is a game changer for delivering reliable, enterprise-scale AI" Sarah Sachs | AI Lead at Notion “Fireworks has been an amazing partner getting our Fast Apply and Copilot++ models running performantly. They exceeded other competitors we reviewed on performance. After testing their quantized model quality for our use cases, we have found minimal degradation. Fireworks helps implement task specific speed ups and new architectures, allowing us to achieve bleeding edge performance!” Sualeh Asif | CPO at Cursor "We've had a really great experience working with Fireworks to host open source models, including SDXL, Llama, and Mistral. After migrating one of our models, we noticed a 3x speedup in response time, which made our app feel much more responsive and boosted our engagement metrics." Spencer Chan | Product Lead at Quora Case Study Sentient Achieved 50% Higher GPU Throughput with Sub-2s Latency Sentient waitlisted 1.8M users in 24 hours, delivering sub-2s latency across 15-agent workflows with 50% higher throughput per GPU and zero infra sprawl, all powered by Fireworks Read the Case Study 50% Higher throughput per GPU What’s new in our blog 10/20/2025 Fireworks and AMD partner to power the next gen of AI infrastructure on AMD Instinct™ GPUs 10/15/2025 LLM on the edge: Model picking with Fireworks Eval Protocol + Ollama 10/9/2025 Announcing Embeddings and Reranking On Fireworks AI Start building today Instantly run popular and specialized models. Get Started Talk to an expert