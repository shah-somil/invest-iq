Tinker - Thinking Machines Lab THINKING MACHINES TinkerToy Computer invented by Daniel Hillis and Brian Silverman Tinker is a training API for researchers Control every aspect of model training and fine-tuning while we handle the infrastructure. Sign up Sign in Docs Your ideas in four functions forward_backward Perform a forward pass and a backward pass, accumulating the gradient. optim_step Update weights based on the accumulated gradient. sample Generate tokens for interaction, evaluation, or RL actions. save_state Save training progress for resumption. Supported models QWEN Qwen3-4B-Instruct-2507 Dense Qwen3-8B-Base Dense Qwen3-8B Dense Qwen3-30B-A3B-Base MoE Qwen3-30B-A3B MoE Qwen3-30B-A3B-Instruct-2507 MoE Qwen3-VL-30B-A3B-Instruct MoE Qwen3-32B Dense Qwen3-235B-A22B-Instruct-2507 MoE Qwen3-VL-235B-A22B-Instruct MoE LLAMA Llama-3.2-1B Dense Llama-3.2-3B Dense Llama-3.1-8B Dense Llama-3.1-8B-Instruct Dense Llama-3.1-70B Dense Llama-3.3-70B-Instruct Dense GPT-OSS GPT-OSS-120B MoE GPT-OSS-20B MoE DEEPSEEK DeepSeek-V3.1 MoE DeepSeek-V3.1-Base MoE MOONSHOT Kimi-K2-Thinking MoE Tinker uses LoRA LoRA fine-tunes models by training a small add-on instead of changing all the original weights. Read blog post Tinker lets researchers focus on datasets, algorithms, and environments without the complexities of compute and infrastructure. Tyler Griggs Tinker lets us focus on the research, rather than spending time on engineering overhead. That's something no amount of raw GPU credits can substitute. Ziran Yang, Yong Lin, Chi Jin The training infrastructure has been abstracted away, which makes focusing on our data and evals far easier. Tinker has made it easy to jump into RL work. Jason Liu Tinker has been reliable for quickly iterating without worrying about hardware or infrastructure. Eric Gan FAQs How do I access Tinker? Join here . If you're a university or organization looking for wide scale access, contact [email protected] . What is Tinker and who is it for? Tinker is a flexible API for efficiently fine-tuning open source models with LoRA. It's designed for researchers and developers who want flexibility and full control of their data and algorithms without worrying about infrastructure management. What is LoRA and why does Tinker use it? LoRA is an efficient approach to fine-tuning that trains a streamlined adapter instead of updating all base model weights. Our research demonstrates that with the right setup, LoRA matches the learning performance of full fine-tuning while providing more flexibility and requiring less compute. Do I need to manage training infrastructure? Tinker handles scheduling, tuning, resource management, and infrastructure reliability so you can focus on the training data and algorithms. Behind the scenes, Tinker orchestrates distributed training on powerful GPU clusters for efficient utilization. What do I need to start training? A dataset of supervised learning examples or reinforcement learning environments. After picking a base model to train on, the Tinker API provides simple functions to compute gradients, update the weights, and sample outputs from the trained model. See our cookbook for examples to get started. What models can I train with Tinker? Tinker is currently available for a broad selection of open-source models, ranging from compact models like Llama-3.2-1B to large MoEs like Qwen3-235B-A22B-Instruct. We plan to expand our model lineup with even more choices soon. How do you handle my training data? Your data is used solely to fine-tune your models. We do not use your data to train our own models. Can I download my model weights? Yes, we have an API endpoint that lets you download any checkpoint you've saved. How much do I have to pay to use Tinker? Tinker uses a pricing plan that reflects compute usage. All prices are in USD per million tokens. Model Prefill Sample Train Qwen3-4B-Instruct-2507 $0.07 $0.22 $0.22 Qwen3-8B $0.13 $0.40 $0.40 Qwen3-30B-A3B $0.12 $0.30 $0.36 Qwen3-VL-30B-A3B-Instruct $0.18 $0.44 $0.53 Qwen3-32B $0.49 $1.47 $1.47 Qwen3-235B-Instruct-2507 $0.68 $1.70 $2.04 Qwen3-VL-235B-A22B-Instruct $1.02 $2.56 $3.07 Llama-3.2-1B $0.03 $0.09 $0.09 Llama-3.2-3B $0.06 $0.18 $0.18 Llama-3.1-8B $0.13 $0.40 $0.40 Llama-3.1-70B $1.05 $3.16 $3.16 DeepSeek-V3.1 $1.13 $2.81 $3.38 GPT-OSS-120B $0.18 $0.44 $0.52 GPT-OSS-20B $0.12 $0.30 $0.36 Kimi-K2-Thinking $0.98 $2.44 $2.93