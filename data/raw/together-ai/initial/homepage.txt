Together AI | The AI Native Cloud Build on the AI Native Cloud Engineered for AI natives, powered by cutting-edge research Start building now Contact sales The Together AI Platform Accelerate training, fine-tuning and inference on performance-optimized GPU clusters Reliable at production scale Built for scale, with customers going to trillions of tokens in a matter of hours without any depletion in experience. Industry-leading unit economics Continuously optimizing across inference and training to keep improving performance, delivering better total cost of ownership. Frontier AI systems research Proven infra and research teams ensure the latest models, hardware, and techniques are made available on day 1. Full stack development for AI â native apps Model Library Inference Fine-Tuning Pre-Training GPU Clusters Model Library Model Library Evaluate and build with open-source and specialized models for chat, images, videos, code, and more. Migrate from closed models with OpenAI-compatible APIs. Start building now together.ai Chat Kimi K2 Thinking Â â New Chat Qwen3 235B A22B Instruct 2507 FP8 Â â Chat gpt-oss-120B Â â New Chat DeepSeek-V3.1 Â â New Video Sora 2 Pro Â â New Chat GLM-4.6 Â â New Image Nano Banana Pro (Gemini 3 Pro Image) Â â New Chat Apriel-1.6-15B-Thinker Â â Free Inference Inference Reliably deploy models with unmatched price-performance at scale. Benefit from inference-focused innovations like the ATLAS speculator system and Together Inference Engine. Deploy on hardware of choice, such as NVIDIA GB200 NVL72 and GB300 NVL72. Learn more Fine-Tuning Fine-Tuning Fine-tune open-source models with your data to create task-specific, fast, and cost-effective models that are 100% yours. Easily deploy into production through Together AI's highly performant inference stack. Learn more Pre-Training Pre-Training Securely and cost effectively train your own models from the ground up, leveraging research breakthroughs such as Together Kernel Collection (TKC) for reliable and fast training. Contact us GPU Clusters GPU Clusters Scale globally with our fleet of data centers (DCs) across the globe. These DCs feature frontier hardware such as NVIDIA GB200 NVL72 and GB300 NVL72. Developers can go from self-serve instant clusters to custom AI factories for high-scale workloads. Learn more Industry leading AI research and open-source contributions FlashAttention Mixture of Agents Dragonfly Red Pajama Datasets DeepCoder Open Deep Research Flash Decoding Open Data ScientistÂ Agent Customer stories AI-native companies partner with Together AI to build the next generation of apps How Hedra Scales Viral AI Video Generation with 60% Cost Savings From AWS to Together Dedicated Endpoints: Arcee AI's journey to greater inference flexibility When Standard Inference Frameworks Failed, Together AI Enabled 5x Performance Breakthrough View all stories Proven results Get to market faster and save costs with breakthrough innovations Faster Inference 3.5x Faster Training 2.3x Lower Cost 20% Network Compression 117x Resources Whatâs new Events Research Inference Together AI delivers fastest inference for the top open-source models Learn More Model Library FLUX.2: Multi-reference image generation now available on Together AI Learn More Model Library Expanding Together AI Model Library into multimedia generation with 40+ new image and video models Learn More Event NeurIPS 2025 Learn More Virtual How advanced tool calling transforms agentic use cases Learn more In-Person ST. LOUIS SuperComputing 2025 Learn more In-Person Helsinki, Finland SLUSH 2025 Learn more In-Person SAN DIEGO NeurIPS 2025 Learn more Research AdapTive-LeArning Speculator System (ATLAS): A New Paradigm in LLM Inference via Runtime-Learning Accelerators Learn more Research Large Reasoning Models Fail to Follow Instructions During Reasoning: A Benchmark Study Learn more Research Back to The Future: Evaluating AI Agents on Predicting Future Events Learn more Research DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL Learn more Start running inference with the best price-performance at scale Explore our model library Subscribe to newsletter Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. Products Solutions Research Blog About Careers Pricing Contact Support Status Trust Center Â© 2025 San Francisco, CA 94114 Consent Preferences Cookie Policy Privacy policy Terms of service